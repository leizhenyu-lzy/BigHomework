{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['font.serif'] = ['SimHei']\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report, roc_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unusedFeatureList = []\n",
    "featureMostfreqValueDict = {}\n",
    "\n",
    "badFeatureMaxMissingSample = 500  # 若某个特征缺失的样本数量超过该值，认定为坏特征\n",
    "badSampleMaxMissingFeature = 10  # 若某个样本缺失的特征超过该值，认定为坏样本\n",
    "\n",
    "trainDatasetPath = 'dataset/train.csv'\n",
    "evalDatasetPath = 'dataset/eval.csv'\n",
    "testDatasetPath = 'dataset/test.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDatasetDFOrigin = pd.read_csv(filepath_or_buffer=trainDatasetPath, header=0, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countDFNull(aimDF):\n",
    "    nullAmount = aimDF.isnull().sum().sum()\n",
    "    # print(\"Null数量 : \", nullAmount)\n",
    "    return nullAmount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79786, 54)\n",
      "361556\n"
     ]
    }
   ],
   "source": [
    "trainDatasetDF = pd.read_csv(filepath_or_buffer=trainDatasetPath, header=0, index_col=None)\n",
    "print(trainDatasetDF.shape)\n",
    "print(countDFNull(trainDatasetDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type :  <class 'pandas.core.series.Series'>\n",
      "缺失500个以上样本的特征数量 :  13\n",
      "{'lartpc': 79508, 'larrout': 79277, 'occutc': 78639, 'v2': 73891, 'v1': 14997, 'pr1': 10096, 'pr': 9672, 'circ': 4255, 'voie': 3712, 'vma': 1287, 'vosp': 1034, 'nbv': 1019, 'adr': 808}\n"
     ]
    }
   ],
   "source": [
    "# 对所有特征（每一列）进行null值统计\n",
    "trainFeatureNullSeries = trainDatasetDF.isnull().sum().sort_values(ascending=False)  # 降序排列\n",
    "print(\"type : \", type(trainFeatureNullSeries))\n",
    "# averageTrainFeatureNull = trainFeatureNullSeries.sum()/len(trainFeatureNullSeries)\n",
    "# print(\"averageTrainFeatureNull : \", averageTrainFeatureNull)\n",
    "trainFeatureNullDict = trainFeatureNullSeries.to_dict()\n",
    "badTrainFeatureDict = {key:trainFeatureNullDict[key] for key in trainFeatureNullDict if trainFeatureNullSeries[key] > badFeatureMaxMissingSample}\n",
    "print(\"缺失%d个以上样本的特征数量 : \"%badFeatureMaxMissingSample, len(badTrainFeatureDict))\n",
    "print(badTrainFeatureDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lartpc', 'larrout', 'occutc', 'v2', 'v1', 'pr1', 'pr', 'circ', 'voie', 'vma', 'vosp', 'nbv', 'adr', 'lartpc', 'larrout', 'occutc', 'v2', 'v1', 'pr1', 'pr', 'circ', 'voie', 'vma', 'vosp', 'nbv', 'adr']\n",
      "(79786, 41)\n"
     ]
    }
   ],
   "source": [
    "unusedFeatureList.extend(badTrainFeatureDict.keys())\n",
    "print(unusedFeatureList)\n",
    "trainDatasetDF.drop(columns=badTrainFeatureDict.keys(), inplace=True)\n",
    "print(trainDatasetDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type :  <class 'pandas.core.series.Series'>\n",
      "缺失10个以上特征的样本数量 :  163\n",
      "(79623, 41)\n"
     ]
    }
   ],
   "source": [
    "trainSampleNullSeries = trainDatasetDF.T.isnull().sum().sort_values(ascending=False) # 倒序排列\n",
    "trainSampleNullDict = trainSampleNullSeries.to_dict()\n",
    "print(\"type : \", type(trainSampleNullSeries))\n",
    "\n",
    "badTrainSampleDict = {key:trainSampleNullDict[key] for key in trainSampleNullDict if trainSampleNullDict[key] > badSampleMaxMissingFeature}\n",
    "print(\"缺失%d个以上特征的样本数量 : \"%badSampleMaxMissingFeature, len(badTrainSampleDict))\n",
    "trainDatasetDF.drop(index=badTrainSampleDict.keys(), inplace=True)\n",
    "print(trainDatasetDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n"
     ]
    }
   ],
   "source": [
    "print(countDFNull(trainDatasetDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tooMuchValueFeatureThreshold = 300  # 如果特征的可能的指多于该数，认定为没有参考性\n",
    "tooLessValueFeatureThreshold = 2  # 如果特征的可能的指少于该数，认定为没有参考性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Num_Acc': 48304, 'jour': 31, 'mois': 12, 'an': 1, 'hrmn': 1347, 'lum': 5, 'dep': 107, 'com': 10018, 'agg': 2, 'int': 9, 'atm': 10, 'col': 8, 'lat': 46055, 'long': 46428, 'catr': 8, 'prof': 4, 'plan': 4, 'surf': 9, 'infra': 10, 'situ': 7, 'id_vehicule': 65846, 'num_veh': 27, 'place': 10, 'catu': 3, 'grav': 4, 'sexe': 2, 'an_nais': 103, 'trajet': 8, 'secu1': 11, 'secu2': 11, 'secu3': 10, 'locp': 11, 'actp': 13, 'etatp': 4, 'senc': 5, 'catv': 31, 'obs': 19, 'obsm': 8, 'choc': 11, 'manv': 28, 'motor': 8}\n",
      "tooMuchValueFeatureList :  ['Num_Acc', 'hrmn', 'com', 'lat', 'long', 'id_vehicule']\n",
      "tooLessValueFeatureList :  ['an']\n"
     ]
    }
   ],
   "source": [
    "featureValueCountDict = {}\n",
    "# 输出各个特征值对应的特征数量\n",
    "for loopIdx, colName in enumerate(trainDatasetDF):\n",
    "    tempSeries = trainDatasetDF[colName]\n",
    "    tempSeriesValueCountDict = tempSeries.value_counts().to_dict()\n",
    "    # print(\"特征\", colName, \"共有%d个可能值\"%len(tempSeriesValueCountDict))\n",
    "    featureValueCountDict[colName] = len(tempSeriesValueCountDict)\n",
    "\n",
    "print(featureValueCountDict)\n",
    "\n",
    "tooMuchValueFeatureList = [feature for feature in featureValueCountDict if featureValueCountDict[feature] > tooMuchValueFeatureThreshold]\n",
    "tooLessValueFeatureList = [feature for feature in featureValueCountDict if featureValueCountDict[feature] < tooLessValueFeatureThreshold]\n",
    "\n",
    "print(\"tooMuchValueFeatureList : \", tooMuchValueFeatureList)\n",
    "print(\"tooLessValueFeatureList : \", tooLessValueFeatureList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unusedFeatureList :  ['lartpc', 'larrout', 'occutc', 'v2', 'v1', 'pr1', 'pr', 'circ', 'voie', 'vma', 'vosp', 'nbv', 'adr', 'lartpc', 'larrout', 'occutc', 'v2', 'v1', 'pr1', 'pr', 'circ', 'voie', 'vma', 'vosp', 'nbv', 'adr', 'Num_Acc', 'hrmn', 'com', 'lat', 'long', 'id_vehicule', 'an']\n"
     ]
    }
   ],
   "source": [
    "unusedFeatureList.extend(tooMuchValueFeatureList)\n",
    "unusedFeatureList.extend(tooLessValueFeatureList)\n",
    "\n",
    "print(\"unusedFeatureList : \", unusedFeatureList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79623, 34)\n",
      "264\n"
     ]
    }
   ],
   "source": [
    "trainDatasetDF.drop(columns=tooMuchValueFeatureList, inplace=True)\n",
    "trainDatasetDF.drop(columns=tooLessValueFeatureList, inplace=True)\n",
    "print(trainDatasetDF.shape)\n",
    "print(countDFNull(trainDatasetDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Num_Acc': 201900049689, 'jour': 18.0, 'mois': 7.0, 'an': 2019.0, 'hrmn': 0.75, 'lum': 1.0, 'dep': '75', 'com': '75116', 'agg': 2.0, 'int': 1.0, 'atm': 1.0, 'col': 3.0, 'adr': 'AUTOROUTE A86', 'lat': 431213200.0, 'long': 59533100.0, 'catr': 4.0, 'voie': '1', 'v1': 0.0, 'v2': 'D', 'circ': 2.0, 'nbv': 2.0, 'vosp': 0.0, 'prof': 1.0, 'pr': 0.0, 'pr1': 0.0, 'plan': 1.0, 'lartpc': 0.0, 'larrout': 7.0, 'surf': 1.0, 'infra': 0.0, 'situ': 1.0, 'vma': 50.0, 'id_vehicule': '138\\xa0212\\xa0691', 'num_veh': 'A01', 'place': 1, 'catu': 1, 'grav': 1, 'sexe': 1, 'an_nais': 2000, 'trajet': 5, 'secu1': 1, 'secu2': 0, 'secu3': -1, 'locp': 0, 'actp': '0', 'etatp': -1, 'senc': 1, 'catv': 7, 'obs': 0, 'obsm': 2, 'choc': 1, 'manv': 1, 'motor': 1, 'occutc': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# 求原始各列众数\n",
    "trainDatasetOriginModeDict = {}\n",
    "for colName in trainDatasetDFOrigin:\n",
    "    trainDatasetOriginModeDict[colName] = trainDatasetDFOrigin[colName].mode()[0]\n",
    "    print(trainDatasetOriginModeDict)\n",
    "print(trainDatasetOriginModeDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dep', 'num_veh', 'actp']\n"
     ]
    }
   ],
   "source": [
    "objectFeatureList = []\n",
    "for colName in trainDatasetDF:\n",
    "    if trainDatasetDF[colName].dtype == 'object':\n",
    "        objectFeatureList.append(colName)\n",
    "print(objectFeatureList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lartpc', 'larrout', 'occutc', 'v2', 'v1', 'pr1', 'pr', 'circ', 'voie', 'vma', 'vosp', 'nbv', 'adr', 'lartpc', 'larrout', 'occutc', 'v2', 'v1', 'pr1', 'pr', 'circ', 'voie', 'vma', 'vosp', 'nbv', 'adr', 'Num_Acc', 'hrmn', 'com', 'lat', 'long', 'id_vehicule', 'an', 'dep', 'num_veh', 'actp', 'dep', 'num_veh', 'actp']\n",
      "(79623, 31)\n"
     ]
    }
   ],
   "source": [
    "unusedFeatureList.extend(objectFeatureList)\n",
    "print(unusedFeatureList)\n",
    "trainDatasetDF.drop(columns=objectFeatureList, inplace=True)\n",
    "print(trainDatasetDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n"
     ]
    }
   ],
   "source": [
    "print(countDFNull(trainDatasetDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jour': 0, 'mois': 0, 'lum': 0, 'agg': 0, 'int': 0, 'atm': 0, 'col': 0, 'catr': 0, 'prof': 22, 'plan': 13, 'surf': 27, 'infra': 63, 'situ': 139, 'place': 0, 'catu': 0, 'grav': 0, 'sexe': 0, 'an_nais': 0, 'trajet': 0, 'secu1': 0, 'secu2': 0, 'secu3': 0, 'locp': 0, 'etatp': 0, 'senc': 0, 'catv': 0, 'obs': 0, 'obsm': 0, 'choc': 0, 'manv': 0, 'motor': 0}\n"
     ]
    }
   ],
   "source": [
    "print(trainDatasetDF.isnull().sum().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type :  <class 'pandas.core.series.Series'>\n",
      "缺失0个以上特征的样本数量 :  211\n"
     ]
    }
   ],
   "source": [
    "finalBadSampleDict = {}\n",
    "trainSampleNullSeries = trainDatasetDF.T.isnull().sum().sort_values(ascending=False) # 倒序排列\n",
    "trainSampleNullDict = trainSampleNullSeries.to_dict()\n",
    "print(\"type : \", type(trainSampleNullSeries))\n",
    "finalBadSampleDict = {key:trainSampleNullDict[key] for key in trainSampleNullDict if trainSampleNullDict[key] > 0}\n",
    "print(\"缺失%d个以上特征的样本数量 : \"%0, len(finalBadSampleDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79412, 31)\n"
     ]
    }
   ],
   "source": [
    "# 删除选出的bad sample整行\n",
    "trainDatasetDF.drop(index=finalBadSampleDict.keys(), inplace=True)\n",
    "print(trainDatasetDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countDFNull(trainDatasetDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28143\n",
      "1768\n",
      "10579\n",
      "27010\n"
     ]
    }
   ],
   "source": [
    "trainSplitDF, testSplitDF = train_test_split(trainDatasetDF, test_size=0.15, random_state=42)\n",
    "print(len(trainSplitDF.groupby('grav').get_group(1)))\n",
    "print(len(trainSplitDF.groupby('grav').get_group(2)))\n",
    "print(len(trainSplitDF.groupby('grav').get_group(3)))\n",
    "print(len(trainSplitDF.groupby('grav').get_group(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSplitX = trainSplitDF.drop(columns='grav')\n",
    "trainSplitY = trainSplitDF.loc[:,'grav']\n",
    "testSplitX = testSplitDF.drop(columns='grav')\n",
    "testSplitY = testSplitDF.loc[:,'grav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67500, 30)\n",
      "(67500,)\n",
      "(11912, 30)\n",
      "(11912,)\n"
     ]
    }
   ],
   "source": [
    "print(trainSplitX.shape)\n",
    "print(trainSplitY.shape)\n",
    "print(testSplitX.shape)\n",
    "print(testSplitY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "\n",
    "preprocessor = make_pipeline(StandardScaler())\n",
    "RandomPipeline = make_pipeline(preprocessor,RandomForestClassifier(random_state=0))\n",
    "AdaPipeline = make_pipeline(preprocessor,AdaBoostClassifier(random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6756212222968435\n"
     ]
    }
   ],
   "source": [
    "RandomPipeline.fit(trainSplitX,trainSplitY)\n",
    "predTestSplitY = RandomPipeline.predict(testSplitX)\n",
    "print(accuracy_score(testSplitY, predTestSplitY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8809126b2e3f6bd67afd8dec0aaf136102c3339cf179547b748c69a78a732e29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
